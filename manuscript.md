---
bibliography: [references.bib]
---

# Introduction

Ecological data as it exists is very difficult to access and reuse  [@Poisot2019EcoDat; @Borregaard2016MorRep; @Gonzalez2015ActSta].
Many databases of ecological and environmental data exist, but synthesizing this data into a single product suitable for analysis often remains tedious, as data
are not in formats that can be easily interfaced.
Yet, assimilation of data at increasing spatial and temporal scales is necessary to answer both pure and applied questions about biodiversity and to quantify human influence on the biosphere [@Hampton2013BigDat; @Giron-Nava2017QuaArg]. Next-generation biodiversity monitoring will necessitate the collection and processing of increasing amounts of data from a variety of sources, requiring increasing flexibility and automation for computationally intensive tasks.
Macroecological data is, by definition, collected across scales which necessitate collaboration across more individuals than can feasibly coordinate with one-another.
The solution to this problem is standardization---developing a robust schema such that ecological data collected in a variety of contexts can be assimilated and integrated into larger scale analysis of ecological processes.

How does standardization solve this problem? Adopting standard representation of
ecological data will have three primary benefits: 1) it will enable new forms of analysis by making combining data from different sources easier, 2) enable continuous integration of new data for next-generation biodiversity monitoring, 3) aid inÂ reproduceability for published results.


> we don't need to use the raw data  
in these methods, we most likely should be using engineered features,  
and writing feature extraction code is orders of magnitudes easier when  
the data are standardized.

Machine learning methods have potential for large scale data
 > ML tools have proven that data are unreasonably  effective, and so we should strive to put as many of them together. The issue of data gaps is also becoming less and less important (what Invenia is doing with their Impute project is amazing, for example), so we really should be merging everything we can. This is the strong argument for a minimum core of interoperable data.

Here we provide case studies of other scientific fields which have greatly reduced the overhead created by data assimilation across large spatial and temporal scales.


# Box: Case Studies of Open Standards

Other scientific disciplines have benefited from open standards for file formats  [@Woolley2020BioMar].

## Case Study One: Bioinformatics

## Case Study Two: Geography

## Case Study Three: Astronomy


# Toward an open standard for ecological data: _EcoJulia_
